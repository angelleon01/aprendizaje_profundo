\documentclass[paper=a4, fontsize=11pt]{scrartcl} % A4 paper and 11pt font size

% ---- Entrada y salida de texto -----

\usepackage[T1]{fontenc} % Use 8-bit encoding that has 256 glyphs
\usepackage[utf8]{inputenc}
\usepackage[a4paper, total={6in, 9in}]{geometry}
%\usepackage{fourier} % Use the Adobe Utopia font for the document - comment this line to return to the LaTeX default

% ---- Idioma --------

\usepackage[spanish, es-tabla]{babel} % Selecciona el español para palabras introducidas automáticamente, p.ej. "septiembre" en la fecha y especifica que se use la palabra Tabla en vez de Cuadro

% ---- Para codigo -----

\usepackage{listings} % Para incluir código
\usepackage{xcolor} % Para definir colores
\usepackage{caption} % Para subtítulos en imágenes y tablas

% Definición de colores para el código
\definecolor{bggray}{rgb}{0.95,0.95,0.95} % Color de fondo para el código
\definecolor{mygreen}{rgb}{0,0.6,0} % Color para comentarios
\definecolor{myblue}{rgb}{0,0,0.8} % Color para palabras clave
\definecolor{mygray}{rgb}{0.5,0.5,0.5} % Color para números

% Configuración del paquete listings para el código
\lstset{
    language=Python, % Lenguaje del código
    backgroundcolor=\color{bggray}, % Fondo gris claro
    basicstyle=\ttfamily\small, % Estilo de fuente básico
    keywordstyle=\color{myblue}, % Color de palabras clave
    commentstyle=\color{mygreen}, % Color de comentarios
    stringstyle=\color{mygray}, % Color de strings
    frame=single, % Marco alrededor del código
    rulecolor=\color{black}, % Color del marco
    showstringspaces=false, % No mostrar espacios en los strings
    breaklines=true, % Permitir salto de línea en código largo
    numbers=left, % Numeración a la izquierda
    numberstyle=\tiny\color{mygray}, % Color y tamaño de los números
}


% ---- Otros paquetes ----


\usepackage{amsmath,amsfonts,amsthm} % Math packages
%\usepackage{graphics,graphicx, floatrow} %para incluir imágenes y notas en las imágenes
\usepackage{graphics,graphicx, float, url} %para incluir imágenes y colocarlas
\usepackage{eurosym}
% Para hacer tablas comlejas
%\usepackage{multirow}
%\usepackage{threeparttable}

%\usepackage{sectsty} % Allows customizing section commands
%\allsectionsfont{\centering \normalfont\scshape} % Make all sections centered, the default font and small caps

%Esto es para hipervinculos
\usepackage[hidelinks]{hyperref}



\usepackage{tikz}
\usetikzlibrary{positioning}
% para grafos


\hypersetup{
    colorlinks=true,
    linkcolor=blue,
    filecolor=blue,      
    urlcolor=blue,
    citecolor=blue,
    pdftitle={Overleaf Example},
    pdfpagemode=FullScreen,
    }

\urlstyle{same}
\usepackage{gensymb}
\usepackage{fancyhdr} % Custom headers and footers
\pagestyle{fancyplain} % Makes all pages in the document conform to the custom headers and footers
\fancyhead{} % No page header - if you want one, create it in the same way as the footers below
\fancyfoot[L]{} % Empty left footer
\fancyfoot[C]{} % Empty center footer
\fancyfoot[R]{\thepage} % Page numbering for right footer
\renewcommand{\headrulewidth}{0pt} % Remove header underlines
\renewcommand{\footrulewidth}{0pt} % Remove footer underlines
\setlength{\headheight}{13.6pt} % Customize the height of the header

\numberwithin{equation}{section} % Number equations within sections (i.e. 1.1, 1.2, 2.1, 2.2 instead of 1, 2, 3, 4)
\numberwithin{figure}{section} % Number figures within sections (i.e. 1.1, 1.2, 2.1, 2.2 instead of 1, 2, 3, 4)
\numberwithin{table}{section} % Number tables within sections (i.e. 1.1, 1.2, 2.1, 2.2 instead of 1, 2, 3, 4)

\setlength\parindent{0pt} % Removes all indentation from paragraphs - comment this line for an assignment with lots of text

\newcommand{\horrule}[1]{\rule{\linewidth}{#1}} % Create horizontal rule command with 1 argument of height

\title{	
\normalfont \normalsize 
\textsc{{\textbf{Aprendizaje Profundo (2024-2025)}} \\ Máster en Robótica e Inteligencia Artificial \\ Universidad de León} \\ [20pt] % Your university, school and/or department name(s)
\horrule{0.5pt} \\[0.4cm] % Thin top horizontal rule
\huge Práctica 2 \\   % The assignment title
\horrule{1.5pt} \\[0.2cm] % Thick bottom horizontal rule
}

\author{Sheila Martínez Gómez\\
Alejandro Mayorga Caro\\
Ángel Morales Romero\\
}
 % Nombre y apellidos
 % Incluye la fecha actual

\begin{document}

\maketitle
\newpage %inserta un salto de página

%\tableofcontents % para generar el índice de contenidos
%\pagebreak

\section{Cuestión 1} 
\begin{gather*}
    w_0 = f^{-1}(x)\\
    w_0(x \cdot e^{x}) = x
\end{gather*}

\begin{gather*}
    w_0 = 0 \Rightarrow w_0(0 \cdot e^{0}) = 0\\ x=0,0000\\    y= 0,0000\\\\
    w_0 = 1 \Rightarrow w_0(1 \cdot e^{1}) = 1\\x=2,7183\\    y= 1,0000\\\\
    w_0 = 2 \Rightarrow w_0(2 \cdot e^{2}) = 1\\ x=14,7181\\    y= 2,0000\\\\  
X= \begin{bmatrix}
0,0000 \\
2,7183 \\
14,7781
\end{bmatrix}
Y= \begin{bmatrix}
0,0000 \\
1,0000 \\
2,0000
\end{bmatrix}
\end{gather*}

\section{Cuestión 2}
\section{Cuestión 3}

Para resolver el ejercicio empezamos por calcular los valores objetivo de $V_D$ en los puntos ($V_{CC} = 3$, $V_{CC} = 6$, $V_{CC} = 9$).


Para ello partiremos de la siguiente ecuación deducida en el enunciado:
\[
V_D = V_{cc} + RI_0 - \eta V_T W_0 \left( \frac{RI_0}{\eta V_T} e^{\frac{V_{cc} + RI_0}{\eta V_T}} \right),
\]

\subsection{Resolución Analítica del Voltaje $V_D$}

Definimos los parámetros:
\[
a = V_{cc} + RI_0, \quad b = \frac{R I_0}{\eta V_T}, \quad \text{y} \quad c = \frac{1}{\eta V_T}.
\]
La solución para $V_D$ en función de $V_{cc}$ es:
\[
V_D = a - \eta V_T W_0 \left( b e^{ac} \right).
\]

Para obtener las soluciones utilizaremos python, declarando las constantes definidas por el enunciado y calculando el resultado del voltaje $V_D$ en cada punto de $V_{CC}$.

\vspace{2mm}
\begin{lstlisting}
# Constantes
I0 = 1e-12
eta = 1
VT = 0.026 
R = 100
Vcc_values = [3, 6, 9]

# Calculo de VD
def calculate_VD_steps(Vcc):
    a = Vcc + R * I0  # Calculamos 'a'
    b = R * I0 / (eta * VT)  # Calculamos 'b'
    exponent = (Vcc + R * I0) / (eta * VT)  # Calculamos el exponent
    W_argument = b * np.exp(exponent)  # Calculamos el argumento de la funcion Lambert
    W_result = lambertw(W_argument).real  # La evaluamos
    VD = a - eta * VT * W_result  # Calculo final de VD
    return VD

# Calculamos el VD de cada Vcc
VD = {Vcc: calculate_VD_steps(Vcc) for Vcc in Vcc_values}
\end{lstlisting}
\vspace{2mm}

Obteniendo de este modo los siguientes valores:
\[
\begin{pmatrix} 3 & 6 & 9\end{pmatrix} \Rightarrow \begin{pmatrix} 0.6212 & 0.6423 & 0.6538 \end{pmatrix}
\]


\subsection{Modelado con Red Neuronal}

Para aproximar el valor de $V_D$ utilizando una red neuronal, empleamos una arquitectura de tres capas, como se puede ver en la figura 3 del enunciado. Comenzamos por declarar las entradas $X = \begin{bmatrix} V_{cc} \end{bmatrix}$ y los pesos y sesgos para cada capa como se detalla a continuación.

\subsubsection{Primera Capa}

La salida de la primera capa es:
\[
S = W_1 \times X + b_1,
\]
donde $W_1$ y $b_1$ son los pesos y el sesgo de la primera capa, definidos como:
\[
W_1 = \begin{bmatrix} 0.05 \\ 0.15 \\ -0.20 \end{bmatrix}, \quad b_1 = \begin{bmatrix} 0.23 \\ -0.10 \\ 0.17 \end{bmatrix}.
\]
Aplicamos la función de activación sigmoide a $S$, obteniendo:
\[
T = \sigma(S) = \frac{1}{1 + e^{-S}}.
\]

\subsubsection{Segunda Capa}

La segunda capa toma como entrada $T$ y produce una salida $U$ mediante la operación:
\[
U = W_2 \times T + b_2,
\]
donde $W_2$ y $b_2$ son los pesos y el sesgo de la segunda capa, dados por:
\[
W_2 = \begin{bmatrix} 0.8 & -0.6 & 0.5 \\ 0.7 & 0.9 & -0.6 \end{bmatrix}, \quad b_2 = \begin{bmatrix} 0.45 \\ -0.34 \end{bmatrix}.
\]
Aplicamos la función de activación ReLU a $U$ para obtener $V$:
\[
V = \text{ReLU}(U) = \max(0, U).
\]

\subsubsection{Capa de Salida}

Finalmente, la capa de salida produce el valor aproximado de $V_D$ como:
\[
Y_{pred} = W_3 \times V + b_3,
\]
donde $W_3 = \begin{bmatrix} 0.8 & 0.8 \end{bmatrix}$ y $b_3 = 0.7$. En esta capa, no aplicamos una función de activación adicional, obteniendo la salida directamente como $Y_{pred}$.

Declarando esto que hemos comentado en python queda de la siguiente manera:

\vspace{2mm}
\begin{lstlisting}
# Datos y objetivos
X = np.array([[3], [6], [9]])   # Entradas Vcc (3 filas x 1 columna)
Y = np.array([[VD[3]], [VD[6]], [VD[9]]])  # Objetivo VD
alpha = 1  # Tasa de aprendizaje

# Pesos y sesgos iniciales
# Primera capa
W1 = np.array([[0.05], [0.15], [-0.20]])  # (3x1)
b1 = np.array([[0.23], [-0.10], [0.17]])  # (3x1)

# Segunda capa
W2 = np.array([[0.8, -0.6, 0.5], [0.7, 0.9, -0.6]])  # (2x3)
b2 = np.array([[0.45], [-0.34]])  # (2x1)

# Capa de salida
W3 = np.array([[0.8, 0.8]])  # (1x2)
b3 = np.array([[0.7]])  # (1x1)

# Funciones de activacion
def sigmoid(x):
    return 1 / (1 + np.exp(-x))

def sigmoid_derivative(x):
    return sigmoid(x) * (1 - sigmoid(x))

def relu(x):
    return np.maximum(0, x)

def relu_derivative(x):
    return (x > 0).astype(float)
\end{lstlisting}
\vspace{10mm}
\begin{lstlisting}
# Propagacion hacia adelante
def forward(X):
    # Primera capa
    S = W1 @ X.T + b1  # Ajustamos para que las dimensiones sean compatibles
    T = sigmoid(S)

    # Segunda capa
    U = W2 @ T + b2
    V = relu(U)

    # Capa de salida
    Y_pred = W3 @ V + b3

    return S, T, U, V, Y_pred
\end{lstlisting}


\subsection{Función de Coste}

La función de coste utilizada para evaluar el rendimiento de la red neuronal es el error cuadrático medio (MSE), dado por:
\[
C = \frac{1}{m} \sum_{i=1}^m (V_D - Y_{pred})^2,
\]
donde $Y_{pred}$ es la predicción de $V_D$ para los voltajes $V_{CC}$ y $V_D$ es el valor real calculado anteriomente.

\vspace{2mm}
\begin{lstlisting}
# Coste (MSE)
def compute_cost(Y, Y_pred):
    m = Y.shape[1]
    cost = np.sum((Y - Y_pred) ** 2) / m
    return cost
\end{lstlisting}

\section{Propagación hacia Atrás}

Para ajustar los pesos y sesgos de la red, calculamos las derivadas parciales de la función de coste con respecto a cada parámetro, usando el método de retropropagación.

\subsubsection{Derivadas en la Capa de Salida}

\[
\frac{\partial C}{\partial Y_{pred}} = Y - Y_{pred},
\]
\[
\frac{\partial C}{\partial W_3} = \frac{1}{m} \left( \frac{\partial C}{\partial Y_{pred}} \right) V^T, \quad \frac{\partial C}{\partial b_3} = \frac{1}{m} \sum_{i=1}^m \frac{\partial C}{\partial Y_{pred}}.
\]

\subsubsection{Derivadas en la Segunda Capa}

\[
\frac{\partial C}{\partial U} = \left( W_3^T \frac{\partial C}{\partial Y_{pred}} \right) \odot \text{ReLU}'(U),
\]
\[
\frac{\partial C}{\partial W_2} = \frac{1}{m} \left( \frac{\partial C}{\partial U} \right) T^T, \quad \frac{\partial C}{\partial b_2} = \frac{1}{m} \sum_{i=1}^m \frac{\partial C}{\partial U}.
\]

\subsubsection{Derivadas en la Primera Capa}

\[
\frac{\partial C}{\partial S} = \left( W_2^T \frac{\partial C}{\partial U} \right) \odot \sigma'(S),
\]
\[
\frac{\partial C}{\partial W_1} = \frac{1}{m} \left( \frac{\partial C}{\partial S} \right) X^T, \quad \frac{\partial C}{\partial b_1} = \frac{1}{m} \sum_{i=1}^m \frac{\partial C}{\partial S}.
\]

\vspace{2mm}
\begin{lstlisting}
# Propagacion hacia atras (backpropagation)
def backward(X, Y, S, T, U, V, Y_pred):
    m = X.shape[1]

    # Derivada de la capa de salida
    dY_pred = Y_pred - Y  # dC/dY_pred
    dW3 = dY_pred @ V.T / m
    db3 = np.sum(dY_pred, axis=1, keepdims=True) / m

    # Derivadas de la segunda capa
    dV = W3.T @ dY_pred
    dU = dV * relu_derivative(U)
    dW2 = dU @ T.T / m
    db2 = np.sum(dU, axis=1, keepdims=True) / m

    # Derivadas de la primera capa
    dT = W2.T @ dU
    dS = dT * sigmoid_derivative(S)
    dW1 = dS @ X / m
    db1 = np.sum(dS, axis=1, keepdims=True) / m

    return dW1, db1, dW2, db2, dW3, db3
\end{lstlisting}
\vspace{2mm}

\subsection{Actualización de Parámetros}

Los pesos y sesgos se actualizan en cada iteración de entrenamiento con la regla de gradiente descendente:
\[
W_k := W_k - \alpha \frac{\partial C}{\partial W_k}, \quad b_k := b_k - \alpha \frac{\partial C}{\partial b_k},
\]
donde $\alpha$ es la tasa de aprendizaje (en este caso 1) y $k$ denota cada capa.

\vspace{2mm}
\begin{lstlisting}
# Actualizacion de pesos
def update_parameters(dW1, db1, dW2, db2, dW3, db3):
    global W1, b1, W2, b2, W3, b3
    W1 -= alpha * dW1
    b1 -= alpha * db1
    W2 -= alpha * dW2
    b2 -= alpha * db2
    W3 -= alpha * dW3
    b3 -= alpha * db3
\end{lstlisting}

\subsection{Conclusión}

Tras ejecutar el código expuesto anteriormente, obtenemos los siguientes valores:
\[
C = 1.0966
\]
\[
Y_{pred} = \begin{pmatrix} 1.6099 & 1.6879 & 1.7579 \end{pmatrix}
\]

Estos valores estan todavía un poco lejos de los valores reales, pero es la primera iteración.  Tras varias iteraciones estos valores deberían ir acercandose más a los valores reales.


\color{red}
\textbf{PROBAR EN DIFERENTES ITERACIONES Y EXPLICAR SI FUNCIONA O NO}



\end{document}
